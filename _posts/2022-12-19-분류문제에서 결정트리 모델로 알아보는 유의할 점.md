---
title : 분류문제에서 결정트리 모델로 알아보는 유의할 점
date : 2022-12-19 20:39:00 +09:00
categories : [머신러닝, 파이썬]
tags : [결정트리, 분류 문제] 
---

# EDA
---

![백문이불여일견](https://user-images.githubusercontent.com/50907018/208410668-cb115f90-eda2-4f4a-bd46-088b379946cb.jpg)

백문이불여일견 이라 했다.

기본적으로 무슨 과제를 해결하든 간에 데이터를 파악하는 작업에 집중해야 한다.

```python
from pandas_profiling import ProfileReport
profile = ProfileReport(train, minimal=True).to_notebook_iframe()
```

보통 중복값, 결측치, 정규화, 표준화 처리 등 문제에 맞게 데이터 전처리를 해줘야 하는데 이것을 하기 전에 데이터에 대한 파악이 필요하다.

이를 파이썬에서는 profileReport 라이브러리를 이용해서 상대적으로 간단하게 진행할 수 있다.

## Pipeline의 이용
---

전처리 과정에는 결측치의 처리나 범주형 자료를 인코딩하는 등의 작업이 필요하다.

### Imputer 
- 결측치를 다루는 방법
- Train 데이터셋을 기준으로 해야함

### Encoder
- 특정 데이터를 ML 모델에 맞게 변형해주는 방법
- Train 데이터셋을 기준으로 해야함


여기서 Imputer와 Encoder 또한 ML 모델과 같이 Train 데이터셋을 기준으로 해야 한다는 점에서 일련의 연결 과정으로 묶을 필요성이 생긴다.

이것을 도와주는 것이 Pipeline이다.

```python
from sklearn.pipeline import make_pipeline

pipe = make_pipeline(
    OneHotEncoder(), 
    SimpleImputer(), 
    StandardScaler(), 
    LogisticRegression(n_jobs=-1)
)
pipe.fit(X_train, y_train)

print('Validation acc : ', pipe.score(X_val, y_val))

y_pred = pipe.predict(X_test)
```

파이썬으로는 이렇게 간단하게 구현할 수 있다.

하이퍼파라미터의 조정을 위해서도 필요하다.

```python
pipe.named_steps
```

named_steps를 통해서 pipeline 내의 각 요소에 접근할 수 있다.

```
{'onehotencoder': OneHotEncoder(cols=['a','b','c']),
 'simpleimputer': SimpleImputer(),
 'standardscaler': StandardScaler(),
 'logisticregression': LogisticRegression(n_jobs=-1)}
```



# 결정트리 모델
---

- 분류와 회귀 문제 둘 다 가능
- 특성에 따라 데이터를 분할하여 나누는 모델
- 데이터의 정규화나 표준화(Scale)를 할 필요 없음
- 특성끼리의 상호작용이 있어도 성능에 문제 없음
- OneHotEncoder의 사용은 성능을 떨어트림
- 앙상블 기법에서 자주 사용됨
-> 랜덤포레스트, GBM(Gradient Boost Machine), AdaBoost 등
- 약한 분류기



## 학습 알고리즘
---
- 지니 불순도 
- 엔트로피
- 정보흭득(분할 후 불순도 - 분할 전 불순도) 가 가장 커지는 방향으로 분할됨



## 파이썬에서 구현
---

[공식 문서](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)


```python

from sklearn.tree import DecisionTreeClassifier

pipe = make_pipeline(
    OneHotEncoder(use_cat_names=True),  
    SimpleImputer(), 
    DecisionTreeClassifier(random_state=1, criterion='entropy')
)

pipe.fit(X_train, y_train)
print('훈련 정확도: ', pipe.score(X_train, y_train))
print('검증 정확도: ', pipe.score(X_val, y_val))

```

단순히 구현은 어렵지 않다.

그러나, DecisionTreeClassifier의 파라미터를 조정해줘야 모델의 성능이 좋아지기 때문에 이에 대한 조정이 필요하다.

### 보통, 과적합을 피하는 방향을 먼저 고려하는 파라미터들 :

-  min_samples_split
-> 노드를 분할하기 위한 최소한의 샘플 데이터 수,
-> 높게 설정할수록 모델의 과적합이 방지된다. 그러나, 너무 높게 설정할 경우 과소적합이 된다.
-   min_samples_leaf
-> 말단 노드(Leaf)가 되기 위한 최소한의 샘플 데이터 수,
-> 비대칭 클래스의 경우 작게 설정한다.
-   max_depth 
-> 트리의 최대 깊이, 높을수록 과적합 확률 증가

주로 먼저 조정하는 파라미터들이다.


## 과적합과 과소적합의 trade-off

---

결국 하이퍼 파라미터 조정을 통해서 과적합이 되거나, 과소적합이 되는데

이 사이의 최적화된 지점을 찾아내야 한다.

그러나 이 과정은 사람이 하기 쉽지 않다.

특정 범위는 시행착오를 통해서 가늠할 수 있으나 정확한 값을 찾으려면 시간이 오래 걸린다.

**이 때문에 하이퍼 파라미터를 찾아주는 기법들을 활용할 필요가 있는 것이다.**

**-> RandomizedSearchCV, GridSearchCV**



