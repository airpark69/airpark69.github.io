---
title : 해커톤대회, NLP 문제 해결방안
date : 2022-12-25 18:21:00 +09:00
categories : [202301해커톤]
tags : [비버웍스, 해커톤, NLP] 
---


# 비버웍스 이커머스 해커톤
---
https://pinkwink.kr/1399

이번에 참가하게 된 대회로, 목표는 NLP를 통한 메뉴명의 분류다.

주최측에서는 DE의 업무라고 명시하고 있으니, 데이터 전처리 과정이 주 목표라고 보면 될 것 같다.



## 그래서 이제 뭐함?

--- 

![근데이제뭐함](https://user-images.githubusercontent.com/50907018/209466288-7aa8ada5-844f-4c26-8c0d-a09627c3e215.jpg)

목표에 맞게 상품명을 분류하면 되겠는데...

문제는 한국어 NLP의 토큰화나 워드 임베딩같은 경우 이미 굉장히 많은 시도가 있었고, 현재 좋은 성능을 보여주는 것은 BERT, GPT같은 트랜스포머 모델이다.

그러나 이 문제는 그 정도의 복잡한 모델과 많은 자원을 들여서 해결할 문제가 아니다.

(라고 생각한다)

## 싼데 비슷한 모델이 필요하다
---
- 자원은 한정적이다.
- 가장 좋은 성능보다도 적당히 좋은 것을 원하는 경우가 있다.
- 자원과 성능에는 trade-off 관계가 있다
- 자원을 아끼는 것과 성능을 만족시키는 이른바 2마리 토끼를 잡아낼 필요가 있다

바로 그것이 싼데비슷한 모델이다.


## BERT가 최고 아님?
---

NLP 다중 분류 문제에 대해서 BERT는 명백하게 좋은 지표를 보여준다.

사전 훈련된 BERT 모델들도 훌륭한 편이기에 막대한 크기의 데이터셋으로 훈련시킬 필요도 없다.

BERT는 신이고 그냥 이것만 쓰면 된다

![갈!](https://user-images.githubusercontent.com/50907018/209467473-5a4b5030-f786-4011-bdb3-13f929676303.png)

**다만**

실무에서 필요로 하는 기준과 목표치가 존재하고, 이는 단순히 더 효율적인 선택지가 아닐 수 있다.

A. Joulin, E. Grave, P. Bojanowski, T. Mikolov, [_Bag of Tricks for Efficient Text Classification_](https://arxiv.org/abs/1607.01759) 에서도 같은 문제를 제기한다.

상대적으로 느린 훈련 속도와 예측 속도, 큰 데이터셋이 필요하다는 부분으로 인해 대안이 필요하다는 것이다.

## 닭 잡는데 소 잡는 칼을 쓴다.

---

위 속담처럼 데이터와 도메인마다 굳이 복잡한 모델이 아니더라도 비슷한 성능을 낼 수 있는 경우가 존재한다.

(물론 속담은 Too Much한 방법을 사용하지 말라는 것이기에 아주 딱 맞는 비유는 아니다.)

데이터에 따라서 최적의 성능을 보여주는 모델은 달라질 수 밖에 없고, 향후 데이터 수집에 따라서 현재 좋은 성능을 보여주는 모델도 성능이 떨어질 가능성도 있다.

많은 변수가 있기에 단순히 좋은게 좋은거지 식으로 모델 선택을 할 수는 없는 것이다.


## 작은 문제를 해결한다
---

이 작은 문제가 어떤 부분이 될 지에 대해서는 좀 더 생각을 해봐야 한다.

하지만, 확실한 것은 데이터, 도메인, 모델 마다 달라지는 문제 해결 방식에 대하여 하나하나 정리하는 시도가 결국 더 좋은 성능을 가져다줄 것이라는 부분이다.

아~ 누가 이런거 다 정리 해놔서 책이 있었으면~ ㅋㅋ








